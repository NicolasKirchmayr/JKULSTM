{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 1: Numpy RNN</h1>\n",
    "\n",
    "Author: Frederik Kratzert\n",
    "\n",
    "**Deadline: 03.11.2019 23:59**\n",
    "\n",
    "\n",
    "In the first assignment, you have to implement the recurrent neural network (RNN) using only NumPy functionality. Concretely, you have to implement the forward and backward pass through the RNN and validate the implementation doing numerical gradient checking. Although you won't most-likely use a self-implemented RNN in real-world applications, this is an excellent task to test and train your understanding of recurrent neural networks in general.\n",
    "\n",
    "The notebook contains the core skelleton of everything you have to implement and you will find comments in the code that point you to the exact locations, where you should implement your code. Furthermore, we added type annotations and documentations, where possible, to help you figuring out what the expected inputs are.  \n",
    "**Note:** You are not allowed to use any other package then the packages imported below. Using other packages will be considered cheating and result in a negativ assignment.\n",
    "\n",
    "When successfully finishing this assignment you will learn:\n",
    "- how to implement the forward and backward to the RNN\n",
    "- how to use gradient checking to verify your implementation\n",
    "\n",
    "\n",
    "Copyright statement:  \n",
    "This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and\n",
    "non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in\n",
    "parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the\n",
    "authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(200,0,0)\">Please fill in</h3> \n",
    "\n",
    "Your Name: **Nicolas Kirchmayr**   \n",
    "Student number: **k11938826** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Excercise 1: Implement Recurrent Neural Network (7pts)</h2>\n",
    "\n",
    "In this excercise you have to implement a recurrent neural network (RNN) in a sequence-to-one setting. That is, only the RNN output of the last time step $T$ is used to calculate the network prediction (see Equations below).\n",
    "\n",
    "Before you start coding note that the script does not contain bias units for notational convenience. In our program, however, we want to use bias units and have to train them using their correct gradients.  \n",
    "**Write down these gradients in the text box below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial \\boldsymbol{b}_s[t]} = \\frac{\\partial L}{\\partial \\boldsymbol{s}[t]}\\frac{\\partial \\boldsymbol{s}[t]}{\\partial \\boldsymbol{b}_s[t]} = \\frac{\\partial L}{\\partial \\boldsymbol{s}[t]} \\\\\n",
    "\\text{since } \\frac{\\partial \\boldsymbol{s}[t]}{\\partial \\boldsymbol{b}_s[t]} = \\frac{\\partial (\\boldsymbol{W}^T \\boldsymbol{x}[t] + \\boldsymbol{R}^T \\boldsymbol{a}[t-1] + \\boldsymbol{b}_s}{\\partial \\boldsymbol{b}_s[t]} = 1\\\\ \n",
    "\\frac{\\partial L}{\\partial \\boldsymbol{b}_y[t]} = \\frac{\\partial L}{\\partial \\boldsymbol{\\hat{y}[t]}} \\frac{\\partial \\boldsymbol{\\hat{y}[t]}}{\\partial \\boldsymbol{b}_y[t]} = \\frac{\\partial L}{ \\partial \\boldsymbol{\\hat{y}[t]}} \\\\\n",
    "\\text{since } \\frac{\\partial \\boldsymbol{\\hat{y}[t]}}{\\partial \\boldsymbol{b}_y[t]} = \\frac{\\partial (\\boldsymbol{V}^T \\boldsymbol{a}[t] + \\boldsymbol{b}_y)}{\\partial \\boldsymbol{b}_y[t]} = 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your implementation, use the provided class skelleton. The inline comments tell you, where and what you have to implement.  \n",
    "**Note two things**:   \n",
    "1) that the final network output should not have any activation function and  \n",
    "2) for this exercise ignore minibatches and implement your RNN to work on a single input sample.\n",
    "\n",
    "Equations of the sequence-to-one RNN:\n",
    "$$\n",
    "        \\boldsymbol{s}[t] = \\boldsymbol{W}^T \\boldsymbol{x}[t] + \\boldsymbol{R}^T \\boldsymbol{a}[t-1] + \\boldsymbol{b}_s\\\\\n",
    "        \\boldsymbol{a}[t] = \\mathrm{tanh}(\\boldsymbol{s}[t]) \\\\\n",
    "        \\boldsymbol{y} = \\boldsymbol{V}^T \\boldsymbol{a}[t=T] + \\boldsymbol{b}_y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    \"\"\"Numpy implementation of sequence-to-one recurrent neural network for regression tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        \"\"\"Initialization \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : int\n",
    "            Number of input features per time step\n",
    "        hidden_size : int\n",
    "            Number of hidden units in the RNN\n",
    "        output_size : int\n",
    "            Number of output units.\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # create and initialize weights of the network\n",
    "        \n",
    "        #####################################################################\n",
    "        # TODO: Create NumPy arrays for each network parameter by replacing #\n",
    "        # the 'None' below. Initialize all matrices from the random uniform #\n",
    "        # distribution between (-0.2, 0.2) and all biases to zero vectors   #\n",
    "        #####################################################################\n",
    "        self.W = np.random.uniform(-0.2, 0.2, (input_size, hidden_size))\n",
    "        self.R = np.random.uniform(-0.2, 0.2, (hidden_size, hidden_size))\n",
    "        self.bs = np.zeros(hidden_size)\n",
    "        self.V = np.random.uniform(-0.2, 0.2, (hidden_size, output_size))\n",
    "        self.by = np.zeros(output_size)\n",
    "\n",
    "        # place holder to store intermediates for backprop\n",
    "        self.a = None\n",
    "        self.y_hat = None\n",
    "        self.grads = {}\n",
    "        self.x = None\n",
    "        \n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass through the RNN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Input sequence(s) of shape [sequence length, number of features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NumPy array containing the network prediction for the input sample.\n",
    "        \"\"\"\n",
    "        #####################################################################\n",
    "        # TODO: Implement the forward pass through the RNN.The              #\n",
    "        # specification of the input and output is given in the doc-string  #\n",
    "        #                                                                   #  \n",
    "        # Hint: you have to store certain intermediate values that you need #\n",
    "        # during the backward pass. Look at the __init__() function to see  #\n",
    "        # which.                                                            # \n",
    "        #                                                                   #\n",
    "        #####################################################################\n",
    "        #get sequence length\n",
    "        seq_length = x.shape[0]\n",
    "        #initialize a to store all values of a(t) including the 0th entry (all zeros vector)\n",
    "        self.a = np.zeros((seq_length + 1, self.hidden_size))\n",
    "        self.x = x\n",
    "        s_0 = np.dot(self.W.T, x[0,:]) + self.bs\n",
    "        self.a[1,:] = np.tanh(s_0)\n",
    "        for i in range(1, seq_length):   \n",
    "            s = np.dot(self.W.T, x[i, :]) + np.dot(self.R.T, self.a[i, :]) + self.bs\n",
    "            self.a[i+1,:] = np.tanh(s)\n",
    "        self.y_hat = np.dot(self.V.T, self.a[-1, :]) + self.by\n",
    "        return self.y_hat\n",
    "\n",
    "    def backward(self, d_loss: np.ndarray) -> Dict:\n",
    "        \"\"\"Calculate the backward pass through the RNN.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        d_loss : np.ndarray\n",
    "            The gradient of the loss w.r.t the network output in the shape [output_size,]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dictionary containing the gradients for each network weight as key-value pair.\n",
    "        \"\"\"\n",
    "        #####################################################################\n",
    "        # TODO: Implement the backward pass through the RNN. The            #\n",
    "        # specification of the input and output is given in the doc-string  #\n",
    "        #                                                                   #\n",
    "        # Hint: You have to store gradients as class attribute which are    #\n",
    "        # needed in the .update() function                                  #\n",
    "        #####################################################################\n",
    "        self.grads['d_V'] = np.outer(self.a[-1,:], d_loss)\n",
    "        # I have troubles using np.dot() for casting (n,) to (n,1) shape\n",
    "        if self.output_size > 1:\n",
    "            delta = np.dot(np.diag(1-self.a[-1,:]**2), np.dot(d_loss.T, self.V.T).T)\n",
    "        else:\n",
    "            delta = np.dot(np.diag(1-self.a[-1,:]**2), np.outer(d_loss.T, self.V.T).T)\n",
    "        self.grads['d_by'] = d_loss\n",
    "        self.grads['d_W'] = np.outer(self.x[-1,:], delta)\n",
    "        self.grads['d_R'] = np.outer(self.a[-2,:], delta)\n",
    "        self.grads['d_bs'] = delta\n",
    "        for i in range(1, self.x.shape[0]):\n",
    "            #there is no loss for time step t != T\n",
    "            delta = np.dot(np.dot(delta.T, self.R.T), np.diag(1-self.a[-i-1,:]**2)).T\n",
    "            self.grads['d_W'] += np.outer(self.x[-i-1,:], delta)\n",
    "            self.grads['d_R'] += np.outer(self.a[-i-2,:], delta)\n",
    "            self.grads['d_bs'] += delta\n",
    "        return self.grads\n",
    "        \n",
    "    def update(self, lr: float):\n",
    "        \"\"\"Update the network parameter.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : float\n",
    "            Learning rate used for the weight update\n",
    "        \"\"\"\n",
    "        if not self.grads:\n",
    "            raise RuntimeError(\"You have to call the .backward() function first\")\n",
    "            \n",
    "        #####################################################################\n",
    "        # TODO: Implement the parameter update using the gradients of the   #\n",
    "        # backward pass.                                                    # \n",
    "        #                                                                   #\n",
    "        # Hint: For certain network parameters you have gradients from each # \n",
    "        # time step. Those gradients over time have to be aggregated.       #  \n",
    "        #####################################################################\n",
    "        self.W += lr*self.grads['d_W']/self.x.shape[0]\n",
    "        self.R += lr*self.grads['d_R']/self.x.shape[0]\n",
    "        self.bs += lr*self.grads['d_bs'].flatten()/self.x.shape[0]\n",
    "        self.V += lr*self.grads['d_V']\n",
    "        self.by += lr*self.grads['d_by']\n",
    "        # reset internal class attributes\n",
    "        self.grads = {}\n",
    "        self.y_hat, self.a = None, None\n",
    "        \n",
    "    def get_weights(self) -> Dict:\n",
    "        \"\"\"Return dictionary containing the weight matrices\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dictionary containing the network weights as key-value pairs.\n",
    "        \"\"\"\n",
    "        return {'W': self.W, 'R': self.R, 'V': self.V, 'bs': self.bs, 'by': self.by}\n",
    "    \n",
    "    def set_weights(self, weights: Dict):\n",
    "        \"\"\"Set the network weights.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : Dict\n",
    "            Dictionary containing one key-value pair for each network weight.\n",
    "        \"\"\"\n",
    "        if not all(name in weights.keys() for name in ['W', 'R', 'V']):\n",
    "            raise ValueError(\"Missing one of 'W', 'R', 'V' keys in the weight dictionary\")\n",
    "        self.W = weights[\"W\"]\n",
    "        self.R = weights[\"R\"]\n",
    "        self.V = weights[\"V\"]\n",
    "        self.bs = weights[\"bs\"]\n",
    "        self.by = weights[\"by\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Excercise 2: Numerical gradient check (3pts)</h2>\n",
    "\n",
    "To validate your implementation, especially the backward pass, use the two-sided gradient approximation given by the equation below.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial f}{\\partial w_i} \\approx \\frac{f(x, w_1,...w_i + \\epsilon,...w_n) - f(x, w_1,...w_i - \\epsilon,...w_n)}{2*\\epsilon}\n",
    "$$\n",
    "\n",
    "Once you calculate the numerical gradient approximation for each weight $w_i$ of the network, we calculate the norm between the numerical gradient and the analytical gradient and check that this difference is smaller than the treshold defined in the function.\n",
    "\n",
    "Again, make use of the function skeleton provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_gradient(model: RNN, x: np.ndarray, eps: float=1e-7) -> Dict:\n",
    "    \"\"\"Implementation of the two-sided numerical gradient approximation\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : RNN\n",
    "        The RNN model object\n",
    "    x : np.ndarray\n",
    "        Input sequence(s) of shape [sequence length, number of features]\n",
    "    eps : float\n",
    "        The epsilon used for numerical gradient approximation\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary containing the numerical gradients for each weight of the RNN. Make sure\n",
    "    to name the dictionary keys like the names of the RNN gradients dictionary (e.g. \n",
    "    'd_R' for the weight 'R')\n",
    "    \"\"\"\n",
    "    #########################################################################\n",
    "    # TODO: Implement the two-sided numerical gradient approximation.       #\n",
    "    # The specifications of the in- and output are defined in the docstring.#                                                  #\n",
    "    #########################################################################    \n",
    "    numerical_grads = {\n",
    "        'd_V': np.zeros_like(model.V),\n",
    "        'd_by': np.zeros_like(model.by),\n",
    "        'd_W': np.zeros_like(model.W),\n",
    "        'd_R': np.zeros_like(model.R),\n",
    "        'd_bs': np.zeros_like(model.bs)\n",
    "    }\n",
    "    for i in range(model.W.shape[0]):\n",
    "        for j in range(model.W.shape[1]):\n",
    "            model.W[i][j] += eps\n",
    "            plus_eps = RNN.forward(model, x)\n",
    "            model.W[i][j] -= 2*eps\n",
    "            minus_eps = RNN.forward(model, x)\n",
    "            model.W[i][j] += eps\n",
    "            grad = np.sum(plus_eps - minus_eps)/(2*eps)\n",
    "            numerical_grads['d_W'][i][j] = grad\n",
    "    for i in range(model.V.shape[0]):\n",
    "        for j in range(model.V.shape[1]):\n",
    "            model.V[i][j] += eps\n",
    "            plus_eps = RNN.forward(model, x)\n",
    "            model.V[i][j] -= 2*eps\n",
    "            minus_eps = RNN.forward(model, x)\n",
    "            model.V[i][j] += eps\n",
    "            grad = np.sum(plus_eps - minus_eps)/(2*eps)\n",
    "            numerical_grads['d_V'][i][j] = grad\n",
    "    for i in range(model.R.shape[0]):\n",
    "        for j in range(model.R.shape[1]):\n",
    "            model.R[i][j] += eps\n",
    "            plus_eps = RNN.forward(model, x)\n",
    "            model.R[i][j] -= 2*eps\n",
    "            minus_eps = RNN.forward(model, x)\n",
    "            model.R[i][j] += eps\n",
    "            grad = np.sum(plus_eps - minus_eps)/(2*eps)\n",
    "            numerical_grads['d_R'][i][j] = grad\n",
    "    for i in range(model.bs.shape[0]):\n",
    "        model.bs[i] += eps\n",
    "        plus_eps = RNN.forward(model, x)\n",
    "        model.bs[i] -= 2*eps\n",
    "        minus_eps = RNN.forward(model, x)\n",
    "        model.bs[i] += eps\n",
    "        grad = np.sum(plus_eps - minus_eps)/(2*eps)\n",
    "        numerical_grads['d_bs'][i] = grad\n",
    "    for i in range(model.by.shape[0]):\n",
    "        model.by[i] += eps\n",
    "        plus_eps = RNN.forward(model, x)\n",
    "        model.by[i] -= 2*eps\n",
    "        minus_eps = RNN.forward(model, x)\n",
    "        model.by[i] += eps\n",
    "        grad = np.sum(plus_eps - minus_eps)/(2*eps)\n",
    "        numerical_grads['d_by'][i] = grad \n",
    "                \n",
    "    return numerical_grads\n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def get_analytical_gradient(model: RNN, x: np.ndarray) -> Dict:\n",
    "    \"\"\"Helper function to get the analytical gradient.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : RNN\n",
    "        The RNN model object\n",
    "    x : np.ndarray\n",
    "        Input sequence(s) of shape [sequence length, number of features]\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary containing the analytical gradients for each weight of the RNN.\n",
    "    \"\"\"\n",
    "    #########################################################################\n",
    "    # TODO: This is only a helper function to extract the analytical        #\n",
    "    # gradient.                                                             #\n",
    "    # The specifications of the in- and output are defined in the docstring.#\n",
    "    #                                                                       #\n",
    "    # Hint: We don't need any loss function at this point. Set e(T) = 1,    #\n",
    "    #       suffices to check the gradient of the RNN backward pass.        #\n",
    "    #########################################################################   \n",
    "    y_hat = RNN.forward(model, x)\n",
    "    grads = RNN.backward(model, np.ones(model.output_size))\n",
    "    RNN.update(model, 0.5)\n",
    "    return grads\n",
    "\n",
    "            \n",
    "def gradient_check(model: RNN, x: np.ndarray, treshold: float = 1e-7):\n",
    "    \"\"\"Perform gradient checking.\n",
    "    \n",
    "    You don't have to do anything in this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : RNN\n",
    "        The RNN model object\n",
    "    x : np.ndarray\n",
    "        Input sequence(s) of shape [sequence length, number of features]\n",
    "    eps : float\n",
    "        The epsilon used for numerical gradient approximation    \n",
    "    \"\"\"\n",
    "    numerical_grads = get_numerical_gradient(model, x)\n",
    "    analytical_grads = get_analytical_gradient(model, x)\n",
    "    for key, num_grad in numerical_grads.items():\n",
    "        difference = np.linalg.norm(num_grad - analytical_grads[key])\n",
    "        if difference < treshold:\n",
    "            print(f\"Gradient check for {key} passed (difference {difference:.3e})\")\n",
    "        else:\n",
    "            print(f\"Gradient check for {key} failed (difference {difference:.3e})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the `gradient_check` function to test your RNN implementation for two different scenarios, which are a single output neuron and multiple output neurons. You don't have to adapt the code below, simply execute the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check with a single output neuron:\n",
      "Gradient check for d_V passed (difference 7.334e-11)\n",
      "Gradient check for d_by passed (difference 2.876e-11)\n",
      "Gradient check for d_W passed (difference 2.122e-10)\n",
      "Gradient check for d_R passed (difference 3.006e-10)\n",
      "Gradient check for d_bs failed (difference 7.925e-01)\n",
      "\n",
      "Gradient check with multiple output neurons:\n",
      "Gradient check for d_V passed (difference 1.854e-10)\n",
      "Gradient check for d_by passed (difference 5.782e-11)\n",
      "Gradient check for d_W passed (difference 1.042e-09)\n",
      "Gradient check for d_R passed (difference 1.428e-09)\n",
      "Gradient check for d_bs passed (difference 5.521e-10)\n"
     ]
    }
   ],
   "source": [
    "print(\"Gradient check with a single output neuron:\")\n",
    "output_size = 1\n",
    "model = RNN(input_size=5, hidden_size=10, output_size=output_size)\n",
    "x = np.random.rand(5, 5)\n",
    "gradient_check(model, x)\n",
    "\n",
    "print(\"\\nGradient check with multiple output neurons:\")\n",
    "output_size = 5\n",
    "model = RNN(input_size=5, hidden_size=10, output_size=output_size)\n",
    "x = np.random.rand(5, 5)\n",
    "gradient_check(model, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Compare the time for gradient computation</h2>\n",
    "Finally, use the code below to investigate the benefit of being able to calculate the exact analytical gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "237 µs ± 20.7 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "analytical_time = %timeit -o get_analytical_gradient(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.8 ms ± 5.31 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "numerical_time = %timeit -o get_numerical_gradient(model, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The analytical gradient computation was 96 times faster\n"
     ]
    }
   ],
   "source": [
    "if analytical_time.average < numerical_time.average:\n",
    "    fraction = numerical_time.average / analytical_time.average\n",
    "    print(f\"The analytical gradient computation was {fraction:.0f} times faster\")\n",
    "else:\n",
    "    fraction = analytical_time.average / numerical_time.average\n",
    "    print(f\"The numerical gradient computation was {fraction:.0f} times faster\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
