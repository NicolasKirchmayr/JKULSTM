{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 2: Solving memory task using the RNN</h1>\n",
    "\n",
    "Author: Frederik Kratzert\n",
    "\n",
    "**Deadline: 17.11.2019 23:59**\n",
    "\n",
    "In this excercise we will use the RNN implemented in the previous assignment to solve the memory task. The task is to remember the first element in the input sequence. That is, given a sequence of random numbers, the RNN should output at the last time step the first element of the input sequence.This can be framed as a regression task. \n",
    "\n",
    "To solve this assignment you have to implement the following things:\n",
    "1. Implement a generator that yields unlimimted training batches for the memory task.\n",
    "2. Implement the mean squared error loss function and its derivative to be able to compute the gradients of the loss w.r.t. the network parameters.\n",
    "3. Implement a learner class to facilitate the model training.\n",
    "4. Train models for different sequence lengths and test up to which sequence length you are able to train the RNN succesfully. Visualize your results.\n",
    "5. Visualize the vanishing gradient problem.\n",
    "\n",
    "Copyright statement:  \n",
    "This  material,  no  matter  whether  in  printed  or  electronic  form,  may  be  used  for  personal  and\n",
    "non-commercial educational use only.  Any reproduction of this manuscript, no matter whether as a whole or in\n",
    "parts, no matter whether in printed or in electronic form, requires explicit prior acceptance of the\n",
    "authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(200,0,0)\">Please fill in</h3> \n",
    "\n",
    "Your Name: **Name**   \n",
    "Student number: **Number** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Tuple, Generator, Dict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Modified RNN implementation</h2>\n",
    "\n",
    "In the last assignment your task was to implement a RNN forward and backward pass that works on a single input sequence. In reality, what you want to do most of the time is training using mini-batches of data samples. That is, you don't do the forward and backward pass for a single sample and then update your weights, but instead do the forward and backward pass for _n_ samples at the same time and calculate the average gradients of the _n_ samples for the weight update.  \n",
    "What you see below is our example solution of last weeks assignment, which will also work for mini-batch training and can be used in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(object):\n",
    "    \"\"\"Numpy implementation of sequence-to-one recurrent neural network for regression tasks.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int):\n",
    "        \"\"\"Initialization \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        input_size : int\n",
    "            Number of input features per time step\n",
    "        hidden_size : int\n",
    "            Number of hidden units in the RNN\n",
    "        output_size : int\n",
    "            Number of output units.\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # create and initialize weights of the network\n",
    "        self.W = np.zeros((input_size, hidden_size))\n",
    "        self.R = np.zeros((hidden_size, hidden_size))\n",
    "        self.bs = np.zeros((hidden_size, 1))\n",
    "        self.V = np.zeros((hidden_size, output_size))\n",
    "        self.by = np.zeros((output_size, 1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "        # place holder to store intermediates for backprop\n",
    "        self.a = None\n",
    "        self.y_hat = None\n",
    "        self.grads = None\n",
    "        self.x = None\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Weight initialization\n",
    "        \n",
    "        Initialize the recurrent weights as identity matrix and the input to hidden\n",
    "        and hidden to output weights using the LeCun initialization.\n",
    "        \"\"\"\n",
    "        # recurrent weights initialized as identity matrix\n",
    "        self.R = np.eye(self.hidden_size)\n",
    "        \n",
    "        # input to hidden and hidden to output initialized with LeCun initialization\n",
    "        gain = np.sqrt(3 / self.input_size)\n",
    "        self.W = np.random.uniform(-gain, gain, self.W.shape)\n",
    "        gain = np.sqrt(3 / self.hidden_size)\n",
    "        self.V = np.random.uniform(-gain, gain, self.V.shape)\n",
    "\n",
    "    def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Forward pass through the RNN.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray\n",
    "            Input sequence(s) of shape [sequence length, batch size, number of features]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        NumPy array containing the network prediction for each sample in the input data.\n",
    "        \"\"\"\n",
    "        seq_length, batch_size, _ = x.shape\n",
    "\n",
    "        self.a = np.zeros((seq_length+1, batch_size, self.hidden_size))\n",
    "\n",
    "        a_0 = np.zeros((batch_size, self.hidden_size))\n",
    "        self.a[-1] = a_0.copy()\n",
    "\n",
    "        for t in range(seq_length):\n",
    "            s_t = np.dot(x[t], self.W) + np.dot(self.a[t-1], self.R) + self.bs.T\n",
    "            a_t = np.tanh(s_t)\n",
    "            self.a[t] = a_t\n",
    "        \n",
    "        y_hat = np.dot(a_t, self.V) + self.by.T\n",
    "        self.y_hat = y_hat.copy()\n",
    "        self.x = x.copy()\n",
    "\n",
    "        return y_hat\n",
    "\n",
    "    def backward(self, d_loss: np.ndarray) -> Dict:\n",
    "        \"\"\"Calculate the backward pass through the RNN.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        d_loss : np.ndarray\n",
    "            The gradient of the loss w.r.t the network output in the shape [batch_size, output_size]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Dictionary containing the gradients for each network weight as key-value pair.\n",
    "        \"\"\"\n",
    "        seq_length, _, _ = self.x.shape\n",
    "\n",
    "        # track gradients of weight matrices\n",
    "        d_V = np.zeros_like(self.V)\n",
    "        d_by = np.zeros_like(self.by)\n",
    "        d_R = np.zeros((seq_length, *self.R.shape))\n",
    "        d_W = np.zeros((seq_length, *self.W.shape))\n",
    "        d_bs = np.zeros((seq_length, *self.bs.shape))\n",
    "        \n",
    "        for t in reversed(range(seq_length)):\n",
    "            if t == seq_length - 1:\n",
    "                d_V = np.dot(self.a[t].T, d_loss)\n",
    "                d_a = np.dot(d_loss, self.V.T)\n",
    "                d_by = d_loss.sum(axis=0).reshape(self.by.shape)\n",
    "            else:\n",
    "                d_a = d_a_next\n",
    "            \n",
    "            d_s = d_a * (1 - self.a[t]*self.a[t])\n",
    "            d_W[t] = np.dot(self.x[t].T, d_s)\n",
    "            d_R[t] = np.dot(self.a[t-1].T, d_s)\n",
    "            d_bs[t] = d_s.sum(axis=0).reshape(d_bs[t].shape)\n",
    "\n",
    "            d_a_next = np.dot(d_s, self.R.T)\n",
    "\n",
    "        self.grads = {'d_V': d_V, 'd_W': d_W, 'd_R': d_R, 'd_bs': d_bs, 'd_by': d_by}\n",
    "        \n",
    "        return self.grads\n",
    "        \n",
    "    def update(self, lr: float):\n",
    "        \"\"\"Updat the network parameter.\n",
    "        \n",
    "        Note: By convention (for numerical stabilization) the average (instead\n",
    "        of the sum) gradient over time is used for updating the recurrent weights.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        lr : float\n",
    "            Learning rate used for the weight update\n",
    "        \"\"\"\n",
    "        if not self.grads:\n",
    "            raise RuntimeError(\"You have to call the .backward() function first\")\n",
    "            \n",
    "        for key, grad in self.grads.items():\n",
    "            if len(grad.shape) == 3:\n",
    "                self.grads[key] = grad.sum(axis=0)\n",
    "                \n",
    "        self.W -= lr*self.grads['d_W']\n",
    "        self.R -= lr*self.grads['d_R']\n",
    "        self.V -= lr*self.grads['d_V']\n",
    "        self.bs -= lr*self.grads['d_bs']\n",
    "        self.by -= lr*self.grads['d_by']\n",
    "\n",
    "        # reset internal class attributes\n",
    "        self.grads = {}\n",
    "        self.y_hat, self.a = None, None\n",
    "        \n",
    "    def get_weights(self) -> Dict:\n",
    "        \"\"\"Return dictionary containing the weight matrices\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Dictionary containing the network weights as key-value pairs.\n",
    "        \"\"\"\n",
    "        return {'W': self.W, 'R': self.R, 'V': self.V, 'bs': self.bs, 'by': self.by}\n",
    "    \n",
    "    def set_weights(self, weights: Dict):\n",
    "        \"\"\"Set the network weights.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : Dict\n",
    "            Dictionary containing one key-value pair for each network weight.\n",
    "        \"\"\"\n",
    "        if not all(name in weights.keys() for name in ['W', 'R', 'V']):\n",
    "            raise ValueError(\"Missing one of 'W', 'R', 'V' keys in the weight dictionary\")\n",
    "        self.W = weights[\"W\"]\n",
    "        self.R = weights[\"R\"]\n",
    "        self.V = weights[\"V\"]\n",
    "        self.bs = weights[\"bs\"]\n",
    "        self.by = weights[\"by\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Excercise 1: Implement Datagenerator</h2>\n",
    "\n",
    "Here you should implement a Python Generator function that, given a `batch size` and a `sequence length`, returns two arrays, `x` and `y`. `x` is a batch of randomly generated input sequences of shape [sequence length, batch size, 1] and `y` the array of target values, which is the first sequence element for each sample in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(batch_size: int, seq_length: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Data generator for memory task\n",
    "    \n",
    "    Note: Implement this function as a Python generator.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int\n",
    "        Number of samples in one batch\n",
    "    seq_length : int\n",
    "        Length of sequence of random numbers\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    x : np.ndarray\n",
    "        Array of shape [sequence length, batch size, 1], where each sample is a sequence\n",
    "        of random generated numbers between -1 and 1.\n",
    "    y : np.ndarray\n",
    "        Array of shape [batch size, 1], where each element i contains the label corresponding\n",
    "        to sample i of the input array. The label is the first element of the sequence.\n",
    "    \n",
    "    \"\"\"\n",
    "    ########################\n",
    "    # Your code comes here #\n",
    "    ########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the code below to validate your implementation. No modifications required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "seq_length = 20\n",
    "data_generator = generate_samples(batch_size=batch_size, seq_length=seq_length)\n",
    "x, y = next(data_generator)\n",
    "assert np.all(x[0,:,:] == y)\n",
    "assert x.shape[0] == seq_length\n",
    "assert x.shape[1] == batch_size\n",
    "assert x.shape[2] == 1\n",
    "assert y.shape[0] == x.shape[1]\n",
    "assert y.shape[1] == 1\n",
    "print(\"Data generator works as expected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Excercise 2: Implement MSE Loss</h2>\n",
    "\n",
    "Since the memory task is a regression type of task, we will use the mean squared error (MSE) as our loss function. Similar to the RNN class object, you should implement the MSE loss as a class having a `forward` and `backward` function (see the class skelleton below).\n",
    "\n",
    "To validate your implementation, use the two-sided numerical gradient approximation again and check against your analytical gradients. \n",
    "\n",
    "Note: The gradient check here is much simpler to implement, compared to the last excercise, since you can apply the small variations directly to the `y_hat` entries (the network predictions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(object):\n",
    "    def __init__(self):\n",
    "        super(MSELoss, self).__init__()\n",
    "        self.y_hat = None\n",
    "        self.y_true = None\n",
    "        \n",
    "    def forward(self, y_hat: np.ndarray, y_true: np.ndarray) -> float:\n",
    "        \"\"\"Calculate the MSE loss\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        y_hat : np.ndarray\n",
    "            Array containing the network predictions of shape [batch_size, 1]\n",
    "        y_true : np.ndarray\n",
    "            Array containing the true values of shape [batch_size, 1]\n",
    "        \n",
    "        Returns:\n",
    "        The mean square error as a floating number.\n",
    "        \"\"\"\n",
    "        #######################################################################\n",
    "        # Your code comes here.                                               # \n",
    "        # Hint: Again, you have to store certain variables that are needed in #\n",
    "        # in the backward pass                                                # \n",
    "        #######################################################################\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self) -> np.ndarray:\n",
    "        \"\"\"Calculate the backward pass of the MSE\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        The gradient w.r.t the network output of shape [batch_size, 1]\n",
    "        \"\"\"\n",
    "        if (self.y_hat is None) or (self.y_true is None):\n",
    "            raise RuntimeError(\"You have to call the .forward() function first\")\n",
    "            \n",
    "        ########################\n",
    "        # Your code comes here #\n",
    "        ########################\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numerical_gradient(loss_obj: MSELoss, y_hat: np.ndarray, y_true: np.ndarray,\n",
    "                           eps: float=1e-7) -> np.ndarray:\n",
    "    \"\"\"Calculate the two-sided numerical gradient approximation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    loss_obj : MSELoss\n",
    "        The MSELoss object\n",
    "    y_hat : np.ndarray\n",
    "        The network prediction of shape [batch size, number of outputs]\n",
    "    y_true : np.ndarray\n",
    "        The true target values of shape [batch size, number of outputs]\n",
    "    eps : float\n",
    "        The pertubations applied to the network predictions, by default 1e-7\n",
    "   \n",
    "    Returns\n",
    "    -------\n",
    "    A NumPy array containing the numerically approximated gradients.\n",
    "    \"\"\"\n",
    "    numerical_gradients = np.zeros_like(y_hat)\n",
    "    #########################################################################\n",
    "    # TODO: Implement the two-sided numerical gradient approximation.       #\n",
    "    # The specifications of the in- and output are defined in the docstring.#                                                  #\n",
    "    #########################################################################    \n",
    "    raise NotImplementedError\n",
    "\n",
    "\n",
    "def get_analytical_gradient(loss_obj: MSELoss, y_hat: np.ndarray, \n",
    "                            y_true: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calculate the analytically derived gradients.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    loss_obj : MSELoss\n",
    "        The MSELoss object\n",
    "    y_hat : np.ndarray\n",
    "        The network prediction of shape [batch size, number of outputs]\n",
    "    y_true : np.ndarray\n",
    "        The true target values of shape [batch size, number of outputs]\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A NumPy array containing the analytically derived gradients.\n",
    "    \"\"\"        \n",
    "    _ = loss_obj.forward(y_hat, y_true)\n",
    "    analytical_grads = loss_obj.backward()\n",
    "            \n",
    "    return analytical_grads\n",
    "\n",
    "            \n",
    "def gradient_check(loss_obj: MSELoss, y_hat: np.ndarray, y_true: np.ndarray, \n",
    "                   treshold: float = 1e-7):\n",
    "    \"\"\"Perform the gradient check.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    loss_obj : MSELoss\n",
    "        The MSELoss object\n",
    "    y_hat : np.ndarray\n",
    "        The network prediction of shape [batch size, number of outputs]\n",
    "    y_true : np.ndarray\n",
    "        The true target values of shape [batch size, number of outputs]\n",
    "    threshold : float\n",
    "        Threshold for the difference between the numerical and analytical gradients.\n",
    "        By default 10e-7.\n",
    "        \n",
    "    \"\"\"            \n",
    "    numerical_grads = get_numerical_gradient(loss_obj, y_hat, y_true)\n",
    "    analytical_grads = get_analytical_gradient(loss_obj, y_hat, y_true)\n",
    "\n",
    "    difference = np.linalg.norm(numerical_grads - analytical_grads)\n",
    "    if difference < treshold:\n",
    "        print(f\"Gradient check passed (difference {difference:.3e})\")\n",
    "    else:\n",
    "        print(f\"Gradient check failed (difference {difference:.3e})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_obj = MSELoss()\n",
    "y_hat = np.random.rand(256, 1)\n",
    "y_true = np.random.rand(256, 1)\n",
    "gradient_check(loss_obj, y_hat, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Excercise 3: Implement Learner class</h2>\n",
    "\n",
    "The task of this excercise is to implement a setup for training the RNN. That is, give the data generator from above, setup a model and connect the outputs to the MSELoss, channel the gradient of the MSELoss into the backward pass of the RNN and monitor the loss during training.\n",
    "\n",
    "For this you should complete the methods of the class `Learner` below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner(object):\n",
    "    def __init__(self, model: RNN, loss_obj: MSELoss, data_generator: Generator):\n",
    "        \"\"\"The initialization method\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model : RNN\n",
    "            An instance of the NumPy RNN implementation.\n",
    "        loss_obj: MSELoss\n",
    "            An instance of the mean squared error loss class.\n",
    "        data_generator : Generator\n",
    "            The data generator function implemented above\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.loss_obj = loss_obj\n",
    "        self.data_generator = data_generator\n",
    "        \n",
    "        self.loss_values = {}\n",
    "        self.y_hats = None\n",
    "        self.y_trues = None\n",
    "        \n",
    "    def train(self, iter_steps: int, lr: float, log_steps: int = 50):\n",
    "        \"\"\"The training method.\n",
    "        \n",
    "        This function implements the training loop for a given number of\n",
    "        iteration steps.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        iter_steps : int\n",
    "            Number of training iteration steps.\n",
    "        lr : float\n",
    "            Learning rate used for the weight update.\n",
    "        log_steps : int\n",
    "            Interval to log the training loss, by default 50.\n",
    "        \"\"\"\n",
    "        if not self.loss_values:\n",
    "            start_step = 0\n",
    "        else:\n",
    "            start_step = list(self.loss_values.keys())[-1]\n",
    "        pbar = tqdm.tqdm_notebook(self.data_generator, total=iter_steps)\n",
    "        for x, y in pbar:\n",
    "            pbar.update()\n",
    "            if pbar.n < iter_steps:\n",
    "                \n",
    "                ##################################################\n",
    "                # Your code comes here:                          #\n",
    "                # Hint: One training step consists of a forward, #\n",
    "                # the backward and the weight update step.       #\n",
    "                ##################################################\n",
    "                \n",
    "                # log loss value\n",
    "                if (pbar.n == 1) or (pbar.n % log_steps == 0):\n",
    "                    self.loss_values[start_step + pbar.n] = loss\n",
    "                    pbar.set_postfix_str(f\"Loss: {loss:5f}\")\n",
    "            else:\n",
    "                tqdm.tqdm.write(\"finished training\")\n",
    "                break\n",
    "    \n",
    "    def make_predictions(self, n_batches: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Calculate predictions for a given number of random batches.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_batches : int\n",
    "            Number of batches to get networks predictions for.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        y_hats : np.ndarray\n",
    "            NumPy array containing the concatenated network predictions for all batches.\n",
    "        y_trues : np.ndarray\n",
    "            NumPy array containing the concatenated true labels for all batches.\n",
    "        \"\"\"\n",
    "        ########################\n",
    "        # Your code comes here #\n",
    "        ########################\n",
    "        raise NotImplementedError\n",
    "                    \n",
    "    def plot_loss(self, figsize: Tuple[float, float]=(10, 8)):\n",
    "        \"\"\"Plot training loss curve.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        figsize : Tuple[float, float]\n",
    "            Matplotlib figure size, by default (10,8)\n",
    "        \"\"\"\n",
    "        if not self.loss_values:\n",
    "            raise RuntimeError(\"You have to train the network first.\")\n",
    "            \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax.plot(list(self.loss_values.keys()), list(self.loss_values.values()))\n",
    "        ax.set_xlabel(\"Weight updates\")\n",
    "        ax.set_ylabel(\"MSE\")\n",
    "    \n",
    "    def scatter_plot(self,figsize: Tuple[float, float]=(10, 8)):\n",
    "        \"\"\"Scatter plot of true vs predicted values.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        figsize : Tuple[float, float]\n",
    "            Matplotlib figure size, by default (10,8)\n",
    "        \"\"\"        \n",
    "        if any(val is None for val in [self.y_hats, self.y_trues]):\n",
    "            raise RuntimeError(\"Call the .make_predictions() method first\")\n",
    "    \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax.scatter(self.y_trues, self.y_hats)\n",
    "        ax.plot([-1,1], [-1,1], '--', color='k', zorder=0)\n",
    "        ax.set_xlabel(\"True values\")\n",
    "        ax.set_ylabel(\"Predicted values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use everything implemented so far to train your first RNN. To do so, execute the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size=1, hidden_size=10, output_size=1)\n",
    "data_generator = generate_samples(batch_size=1024, seq_length=10)\n",
    "\n",
    "learner = Learner(model=model, loss_obj=loss_obj, data_generator=data_generator)\n",
    "learner.train(iter_steps=5000, lr=1e-2, log_steps=50)\n",
    "learner.plot_loss()\n",
    "_ = learner.make_predictions(n_batches=1)\n",
    "learner.scatter_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Excercise 4: Train RNNs</h2>\n",
    "\n",
    "Using everything implemented so far you should now be able to train RNNs fairly easy. Go ahead and train RNNs to solve the memory task with different sequence length. Test what is the maximum sequence length you are able to solve with the RNN. For each setting, train multiple repetitions (e.g. 10), to see the effect of the random initialization and visualize this random variations in your plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Your code comes here #\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the results, including error bars (from the repetitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Your code comes here #\n",
    "########################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Excercise 5: Vanishing Gradient</h2>\n",
    "\n",
    "In the lecture you have heard that the problem of training RNNs on long sequences is called the vanishing gradient problem. Come up with your own ideas how you can visualize this problem. \n",
    "\n",
    "Hint: You could, for example, compare the gradients over time for two different sequence lengths - one sequence length, where the RNN still is able to learn and one, where it does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Your code comes here #\n",
    "########################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
